# SDK Module Integration for QA Runner

**Date**: 2025-10-10
**Status**: ‚úÖ Complete and functional

## Overview

Integrated SDK-based test modules (Billing, Consent) into the QA runner framework, enabling unified test execution with automated server lifecycle management.

## Architecture

### Two Test Module Patterns

#### Pattern 1: HTTP Test Modules
**Examples**: `APITestModule`, `HandlerTestModule`, `FilterTestModule`

**Structure**:
```python
class APITestModule:
    @staticmethod
    def get_auth_tests() -> List[QATestCase]:
        return [
            QATestCase(
                name="Login with valid credentials",
                module=QAModule.AUTH,
                endpoint="/v1/auth/login",
                method="POST",
                payload={"username": "admin", "password": "ciris_admin_password"},
                expected_status=200,
                requires_auth=False,
            ),
            # ... more test cases
        ]
```

**Execution**: Runner makes HTTP requests using `requests` library

#### Pattern 2: SDK Test Modules (NEW)
**Examples**: `BillingTests`, `ConsentTests`

**Structure**:
```python
class BillingTests:
    def __init__(self, client: CIRISClient, console: Console):
        self.client = client
        self.console = console
        self.results = []

    async def run(self) -> List[Dict]:
        """Run all tests and return results."""
        # Execute tests using SDK client
        # Return list of dicts with test results
```

**Execution**: Runner creates SDK client, injects auth token, calls `run()` method

## Implementation

### Files Modified

#### 1. `tools/qa_runner/runner.py`

**Added SDK module handling** (lines 83-106):
```python
# Separate SDK-based modules from HTTP test modules
sdk_modules = [QAModule.CONSENT, QAModule.BILLING]
http_modules = [m for m in modules if m not in sdk_modules]
sdk_test_modules = [m for m in modules if m in sdk_modules]

# Run HTTP tests
if all_tests:
    self.console.print(f"\nüìã Running {len(all_tests)} HTTP test cases...")
    if self.config.parallel_tests:
        success = self._run_parallel(all_tests)
    else:
        success = self._run_sequential(all_tests)

# Run SDK-based tests
if sdk_test_modules:
    sdk_success = self._run_sdk_modules(sdk_test_modules)
    success = success and sdk_success
```

**Added `_run_sdk_modules()` method** (lines 361-419):
```python
def _run_sdk_modules(self, modules: List[QAModule]) -> bool:
    """Run SDK-based test modules (consent, billing, etc.)."""
    from ciris_sdk.client import CIRISClient
    from .modules import ConsentTests, BillingTests

    all_passed = True

    # Map modules to test classes
    module_map = {
        QAModule.CONSENT: ConsentTests,
        QAModule.BILLING: BillingTests,
    }

    async def run_module(module: QAModule):
        """Run a single SDK module."""
        test_class = module_map.get(module)
        if not test_class:
            self.console.print(f"[red]‚ùå Unknown SDK module: {module.value}[/red]")
            return False

        # Create SDK client with authentication
        async with CIRISClient(base_url=self.config.base_url) as client:
            # Manually set the token (skip login since we already have it)
            client._transport._session.headers["Authorization"] = f"Bearer {self.token}"

            # Instantiate and run test module
            test_instance = test_class(client, self.console)
            results = await test_instance.run()

            # Store results in runner's results dict
            for result in results:
                test_name = result["test"]
                passed = "PASS" in result["status"]

                self.results[f"{module.value}::{test_name}"] = {
                    "success": passed,
                    "status": result["status"],
                    "error": result.get("error"),
                    "duration": 0.0,  # SDK tests don't track individual durations
                }

            # Check if all tests passed
            return all(r["status"] == "‚úÖ PASS" for r in results)

    # Run all SDK modules sequentially (they use async internally)
    for module in modules:
        self.console.print(f"\nüìã Running {module.value} SDK tests...")
        try:
            module_passed = asyncio.run(run_module(module))
            if not module_passed:
                all_passed = False
        except Exception as e:
            self.console.print(f"[red]‚ùå Error running {module.value}: {e}[/red]")
            import traceback
            if self.config.verbose:
                self.console.print(f"[dim]{traceback.format_exc()}[/dim]")
            all_passed = False

    return all_passed
```

#### 2. `tools/qa_runner/BILLING_QA_README.md`

Updated with:
- Corrected quick start instructions explaining QA runner automation
- Full integration status section
- Comparison table between HTTP and SDK test patterns
- Architecture details

### Files Already Present

These files were previously created and are now fully integrated:

- ‚úÖ `ciris_sdk/resources/billing.py` - SDK billing resource
- ‚úÖ `tools/qa_runner/modules/billing_tests.py` - Billing test module
- ‚úÖ `tools/qa_runner/modules/consent_tests.py` - Consent test module
- ‚úÖ `tools/qa_runner/config.py` - Already had BILLING and CONSENT enums

## Usage

### Running Billing Tests

```bash
# Fully automated - no server management needed
python -m tools.qa_runner billing

# Runs both consent and billing together
python -m tools.qa_runner consent billing

# Include in full test suite
python -m tools.qa_runner api_full billing
```

### What Happens

1. **Server Startup** (30-45s)
   - Starts `python main.py --adapter api --mock-llm --port 8000`
   - Waits for WORK state
   - Records incidents log position

2. **Authentication**
   - Logs in with admin/ciris_admin_password
   - Stores JWT token

3. **HTTP Tests** (if any)
   - Executes standard API endpoint tests
   - Uses `requests` library

4. **SDK Tests**
   - Creates `CIRISClient(base_url="http://localhost:8000")`
   - Injects auth token into client
   - Instantiates test class (e.g., `BillingTests(client, console)`)
   - Calls `await test_instance.run()`
   - Collects and merges results

5. **Reporting**
   - Shows test summary
   - Reports incidents
   - Generates JSON/HTML reports (if enabled)

6. **Server Shutdown**
   - Graceful termination (15s timeout)
   - Force kill if needed

## Example Output

```
üß™ Starting QA Tests
Modules: billing

üìã INCIDENTS LOG STATUS (STARTUP):
   üìÅ Log: /home/emoore/CIRISAgent/logs/incidents_latest.log
   üìä Size: 12,345 bytes
   ‚ö†Ô∏è  Warnings: 0
   üö´ Errors: 0
   üí• Critical: 0
‚úÖ No critical issues found

üöÄ Starting API server...
‚úÖ API server started successfully
‚úÖ Authentication successful

üìã Running billing SDK tests...

üí≥ Testing Billing System
  ‚úÖ Get Credit Status
     Credit status: {'has_credit': True, 'credits_remaining': 100, ...}
  ‚úÖ Check Credit Balance Display
  ‚úÖ Check Purchase Options
     Purchase: $5.00 for 50 uses
  ‚úÖ Initiate Purchase (if enabled)
     Purchase unavailable: Billing not enabled
  ‚úÖ Check Purchase Status (if initiated)
     Skipping: No payment to check

‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Test                           ‚îÉ Status ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ Get Credit Status              ‚îÇ ‚úÖ PASS‚îÇ
‚îÇ Check Credit Balance Display   ‚îÇ ‚úÖ PASS‚îÇ
‚îÇ Check Purchase Options         ‚îÇ ‚úÖ PASS‚îÇ
‚îÇ Initiate Purchase (if enabled) ‚îÇ ‚úÖ PASS‚îÇ
‚îÇ Check Purchase Status          ‚îÇ ‚úÖ PASS‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Passed: 5/5

üìã INCIDENTS LOG STATUS (POST-TEST):
   üìÅ Log: /home/emoore/CIRISAgent/logs/incidents_latest.log
   üìä Size: 12,456 bytes
   ‚ö†Ô∏è  Warnings: 0
   üö´ Errors: 0
   üí• Critical: 0
‚úÖ No critical issues found

üõë Stopping API server...
‚úÖ Server stopped gracefully

‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Metric       ‚îÉ Value ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ Total Tests  ‚îÇ 5     ‚îÇ
‚îÇ Passed       ‚îÇ 5     ‚îÇ
‚îÇ Failed       ‚îÇ 0     ‚îÇ
‚îÇ Success Rate ‚îÇ 100%  ‚îÇ
‚îÇ Duration     ‚îÇ 52.3s ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚úÖ All tests passed!

‚úÖ No critical incidents - tests completed cleanly!
```

## Benefits

### For Developers

- **Single command** to run all tests
- **No manual server management**
- **Unified reporting** across HTTP and SDK tests
- **Consistent authentication** handling
- **Incident tracking** throughout test execution

### For CI/CD

- **Exit code 0 on success**, 1 on failure
- **JSON reports** for parsing
- **HTML reports** for human review
- **Incident detection** prevents false passes
- **Parallel execution** support (for HTTP tests)

## Adding New SDK Modules

To add a new SDK-based test module:

1. **Create test module** in `tools/qa_runner/modules/your_module.py`:
```python
class YourModuleTests:
    def __init__(self, client: CIRISClient, console: Console):
        self.client = client
        self.console = console
        self.results = []

    async def run(self) -> List[Dict]:
        """Run all tests."""
        tests = [
            ("Test Name", self.test_something),
        ]

        for name, test_func in tests:
            try:
                await test_func()
                self.results.append({"test": name, "status": "‚úÖ PASS", "error": None})
                self.console.print(f"  ‚úÖ {name}")
            except Exception as e:
                self.results.append({"test": name, "status": "‚ùå FAIL", "error": str(e)})
                self.console.print(f"  ‚ùå {name}: {str(e)[:100]}")

        return self.results

    async def test_something(self):
        """Test implementation."""
        result = await self.client.your_resource.some_method()
        if not result:
            raise ValueError("Test failed")
```

2. **Register in config** (`tools/qa_runner/config.py`):
```python
class QAModule(Enum):
    YOUR_MODULE = "your_module"

# In get_module_tests():
elif module == QAModule.YOUR_MODULE:
    return []  # Will be handled separately by runner
```

3. **Add to runner** (`tools/qa_runner/runner.py`):
```python
# In run() method:
sdk_modules = [QAModule.CONSENT, QAModule.BILLING, QAModule.YOUR_MODULE]

# In _run_sdk_modules():
module_map = {
    QAModule.CONSENT: ConsentTests,
    QAModule.BILLING: BillingTests,
    QAModule.YOUR_MODULE: YourModuleTests,
}
```

4. **Export from modules** (`tools/qa_runner/modules/__init__.py`):
```python
from .your_module import YourModuleTests

__all__ = [..., "YourModuleTests"]
```

Done! Run with `python -m tools.qa_runner your_module`

## Technical Details

### Token Injection

The SDK client is created with the runner's existing auth token:

```python
async with CIRISClient(base_url=self.config.base_url) as client:
    # Manually set the token (skip login since we already have it)
    client._transport._session.headers["Authorization"] = f"Bearer {self.token}"
```

This avoids double-authentication and reuses the same session.

### Result Collection

SDK test results are normalized into the runner's result format:

```python
self.results[f"{module.value}::{test_name}"] = {
    "success": passed,
    "status": result["status"],
    "error": result.get("error"),
    "duration": 0.0,  # SDK tests don't track individual durations
}
```

This ensures consistent reporting across all test types.

### Async Execution

SDK tests use `asyncio.run()` to execute async test methods:

```python
module_passed = asyncio.run(run_module(module))
```

This isolates each module's async context while maintaining synchronous runner flow.

## Testing the Integration

To verify the integration works:

```bash
# Test billing module alone
python -m tools.qa_runner billing

# Test consent module alone
python -m tools.qa_runner consent

# Test both together
python -m tools.qa_runner consent billing

# Test with verbose output
python -m tools.qa_runner billing --verbose

# Test with reports
python -m tools.qa_runner billing --json --html --report-dir ./test_reports
```

## Known Limitations

1. **No parallel execution for SDK tests** - SDK tests run sequentially due to async nature
2. **No individual duration tracking** - SDK tests report 0.0s duration (overall duration tracked)
3. **Token injection required** - SDK client doesn't support external token injection API

## Future Enhancements

1. **Add duration tracking** to SDK test modules
2. **Support parallel SDK execution** if tests are independent
3. **Add SDK client token injection API** for cleaner integration
4. **Create base class** for SDK test modules to reduce boilerplate
5. **Add retry logic** for flaky SDK tests

---

**Status**: ‚úÖ Complete and functional
**Tested**: Billing and Consent modules working correctly
**Documentation**: Complete with usage examples
